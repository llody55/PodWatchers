import os
import time
import json
import logging
import threading
from minio import Minio
from minio.error import S3Error
from datetime import datetime
from typing import Dict, List, Optional
from kubernetes import client, config, watch
from kubernetes.client.rest import ApiException
import requests

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ç¯å¢ƒå˜é‡é…ç½®ç±»
class AppConfig:
    def __init__(self):
        self.mute_seconds = int(os.getenv("MUTE_SECONDS", 30)) # é™é»˜æ—¶é—´
        self.ignore_restart_count = int(os.getenv("IGNORE_RESTART_COUNT", 1)) # å¿½ç•¥é‡å¯æ¬¡æ•°
        self.watched_namespaces = self._parse_namespaces(os.getenv("WATCHED_NAMESPACES", "")) # ç›‘æ§çš„å‘½åç©ºé—´
        self.ignore_exit_zero = os.getenv("IGNORE_EXIT_CODE_ZERO", "true").lower() == "true" # å¿½ç•¥é€€å‡ºç ä¸º0çš„é‡å¯
        self.webhook_url = os.getenv("WEBHOOK_URL", "") # ä¼ä¸šå¾®ä¿¡æœºå™¨äººWebhookåœ°å€
        self.cluster_name = os.getenv("CLUSTER_NAME", "default-cluster") # é›†ç¾¤åç§°
        self.log_lines = int(os.getenv("LOG_LINES", 20))  # æ—¥å¿—è¡Œæ•°é…ç½®
        self.log_file_lines = int(os.getenv("LOG_FILE_LINES", 500))  # æ—¥å¿—æ–‡ä»¶è¡Œæ•°é…ç½®
        self.event_entries = int(os.getenv("EVENT_ENTRIES", 5))  # äº‹ä»¶æ¡æ•°é…ç½®
        self.watch_timeout = int(os.getenv("WATCH_TIMEOUT", 300))  # å‘½åç©ºé—´è¶…æ—¶æ—¶é—´é…ç½®

        self.minio_enabled = os.getenv("MINIO_ENABLED", "false").lower() == "true"  # MinIOæ—¥å¿—å­˜å‚¨å¯ç”¨é…ç½®
        self.minio_endpoint = os.getenv("MINIO_ENDPOINT", "minio:9000")  # MinIOæœåŠ¡å™¨åœ°å€
        self.minio_access_key = os.getenv("MINIO_ACCESS_KEY", "minio")  # MinIOè®¿é—®å¯†é’¥
        self.minio_secret_key = os.getenv("MINIO_SECRET_KEY", "minio123")  # MinIOå¯†é’¥
        self.minio_bucket = os.getenv("MINIO_BUCKET", "k8s-logs")  # MinIOå­˜å‚¨æ¡¶åç§°
        self.minio_secure = os.getenv("MINIO_SECURE", "false").lower() == "true"  # MinIOå®‰å…¨è¿æ¥é…ç½®
        self.minio_proxy_endpoint = os.getenv("MINIO_PROXY_ENDPOINT", "")  # MinIOä»£ç†æœåŠ¡å™¨åœ°å€
    
    @staticmethod
    def _parse_namespaces(raw: str) -> List[str]:
        return [ns.strip() for ns in raw.split(",") if ns.strip()] if raw else []

CONFIG = AppConfig()

# å‘Šè­¦çŠ¶æ€ç®¡ç†
class AlertState:
    def __init__(self):
        self.history: Dict[str, datetime] = {}
        self.lock = threading.Lock()
    
    def should_alert(self, pod_uid: str) -> bool:
        with self.lock:
            last_alert = self.history.get(pod_uid)
            if last_alert and (datetime.now() - last_alert).total_seconds() < CONFIG.mute_seconds:
                return False
            self.history[pod_uid] = datetime.now()
            return True

alert_state = AlertState()

def load_k8s_config():
    try:
        config.load_incluster_config()
        logger.info("Loaded in-cluster Kubernetes config")
    except config.ConfigException:
        config.load_kube_config()
        logger.info("Loaded local Kubernetes config")

def get_restart_info(pod: client.V1Pod) -> dict:
    """è·å–é‡å¯ç›¸å…³å®Œæ•´ä¿¡æ¯"""
    if not pod.status.container_statuses:
        return {"count": 0, "exit_code": None, "reasons": []}
    
    restart_count = sum(c.restart_count for c in pod.status.container_statuses)
    exit_code = None
    reasons = []
    last_restart_time = None
    
    for cs in pod.status.container_statuses:
        if cs.last_state.terminated:
            exit_code = exit_code or cs.last_state.terminated.exit_code
            # logger.warning(f"restart time: {cs.last_state.terminated.finished_at}")
            reasons.append({
                "container": cs.name,
                "reason": cs.last_state.terminated.reason,
                "exit_code": cs.last_state.terminated.exit_code
            })
            last_restart_time = cs.last_state.terminated.finished_at
    
    return {
        "count": restart_count,
        "exit_code": exit_code,
        "reasons": reasons,
        "last_restart_time": last_restart_time
    }

def fetch_resource(
    func, 
    resource_type: str,
    **kwargs
) -> Optional[List[dict]]:
    """é€šç”¨èµ„æºè·å–å‡½æ•°"""
    try:
        resp = func(**kwargs)
        return [item.to_dict() for item in resp.items]
    except ApiException as e:
        logger.error(f"Failed to get {resource_type}: {str(e)}")
        return None

def prepare_alert_data(pod: client.V1Pod) -> Optional[dict]:
    """å‡†å¤‡å‘Šè­¦æ•°æ®"""
    restart_info = get_restart_info(pod)
    
    # è¿‡æ»¤æ¡ä»¶
    if CONFIG.ignore_exit_zero and restart_info["exit_code"] == 0:
        logger.debug(f"Ignoring pod {pod.metadata.name} with exit code 0")
        return None
    
    if restart_info["count"] <= CONFIG.ignore_restart_count:
        logger.debug(f"Ignoring pod {pod.metadata.name} with restart count {restart_info['count']}")
        return None
    
    if not alert_state.should_alert(pod.metadata.uid):
        logger.info(f"Pod {pod.metadata.name} in mute period")
        return None
    
    # æ”¶é›†æ•°æ®
    alert_data = {
        "metadata": pod.metadata.to_dict(),
        "restart_info": restart_info,
        "events": fetch_resource(
            client.CoreV1Api().list_namespaced_event,
            "events",
            namespace=pod.metadata.namespace,
            field_selector=f"involvedObject.name={pod.metadata.name}"
        ),
        "logs": fetch_pod_logs(pod.metadata.namespace, pod.metadata.name)
    }
    # æ—¥å¿—ä¸å®Œæ•´æ—¶ï¼Œå°è¯•å†æ¬¡è·å–
    if "unable to retrieve container logs" in alert_data["logs"]:
        logger.info("Retrying to fetch logs...")
        alert_data["logs"] = fetch_pod_logs(pod.metadata.namespace, pod.metadata.name)
    # æ·»åŠ èŠ‚ç‚¹ä¿¡æ¯
    alert_data["node_name"] = pod.spec.node_name if pod.spec else "Unknown"

    return alert_data

def format_events(events: List[dict]) -> str:
    """æ ¼å¼åŒ–äº‹ä»¶ä¿¡æ¯"""
    if not events:
        return "No recent events"
    
    # åˆå¹¶åŒç±»äº‹ä»¶
    event_counts = {}
    for e in events:
        key = (e.get('reason'), e.get('message'))
        event_counts[key] = event_counts.get(key, 0) + 1

    output = []
    for (reason, msg), count in event_counts.items():
        if count > 1:
            output.append(f"Ã— [{count}æ¬¡] {reason}: {msg}")
        else:
            output.append(f"Ã— {reason}: {msg}")
    
    return "\n".join(output[:CONFIG.event_entries])  # é»˜è®¤æ˜¾ç¤ºæœ€å¤š5æ¡å…³é”®äº‹ä»¶

def fetch_pod_logs(namespace: str, name: str) -> str:
    """è·å–å®¹å™¨æ—¥å¿—"""
    try:
        raw_logs = client.CoreV1Api().read_namespaced_pod_log(
            name=name,
            namespace=namespace,
            previous=True
        )
        # æŒ‰è¡Œæ•°æˆªæ–­
        lines = raw_logs.split('\n')
        # logger.warning(f"å†å² Raw logs: {raw_logs}")
        return '\n'.join(lines[-CONFIG.log_file_lines:])
    except ApiException as e:
        logger.error(f"Failed to get previous logs: {str(e)}")
        # å°è¯•è·å–å½“å‰æ—¥å¿—
        try:
            raw_logs = client.CoreV1Api().read_namespaced_pod_log(
                name=name,
                namespace=namespace,
                previous=False
            )
            lines = raw_logs.split('\n')
            # logger.warning(f"å½“å‰ Raw logs: {raw_logs}")
            return '\n'.join(lines[-CONFIG.log_file_lines:]) + "\n[Note: Previous logs unavailable, showing current logs]"
        except ApiException as e2:
            logger.error(f"Failed to get current logs: {str(e2)}")
            return f"Logs unavailable: {e.reason}"

def format_logs(logs: str) -> str:
    """æ ¼å¼åŒ–æ—¥å¿—ä¿¡æ¯"""
    if not logs:
        return "No logs available"
    
    return  logs  # æˆªæ–­é•¿æ—¥å¿—
    # æå–å…³é”®é”™è¯¯è¡Œ
    # error_keywords = ['error', 'exception', 'failed', 'warning']
    # filtered = [line for line in logs.split('\n') 
    #            if any(kw in line.lower() for kw in error_keywords)]
    # logger.warning(f"Filtered logs: {filtered}")
    # return "\n".join(filtered[-10:]) or "No obvious error logs"  # æ˜¾ç¤ºæœ€å¤š3æ¡å…³é”®æ—¥å¿—

# è‡ªå®šä¹‰ JSON åºåˆ—åŒ–å‡½æ•°
def json_serializable(obj):
    """å¤„ç†æ— æ³•ç›´æ¥åºåˆ—åŒ–ä¸º JSON çš„å¯¹è±¡"""
    if isinstance(obj, datetime):
        return obj.isoformat()
    raise TypeError(f"Object of type {type(obj)} is not JSON serializable")

def format_reasons(reasons: List[dict]) -> str:
    """æ ¼å¼åŒ–ç»ˆæ­¢åŸå› """
    if not reasons:
        return "No termination reasons available"
    return "\n".join([
        f"å®¹å™¨ {r['container']}ï¼š{r['reason']} (Exit {r['exit_code']})"
        for r in reasons
    ])

def human_readable_time(seconds: float) -> str:
    """è½¬æ¢å¯è¯»æ—¶é—´"""
    mins, sec = divmod(seconds, 60)
    hrs, min = divmod(mins, 60)
    return f"{int(hrs)}h {int(min)}m {int(sec)}s"

def save_log_to_file(logs: str, pod_name: str) -> str:
    """å°†æ—¥å¿—ä¿å­˜ä¸ºæœ¬åœ°æ–‡ä»¶"""
    try:
        log_dir = "/tmp/logs"
        os.makedirs(log_dir, exist_ok=True)
        log_file = f"{log_dir}/{pod_name}-{datetime.now().strftime('%Y%m%d%H%M%S')}.log"
        
        with open(log_file, 'w') as f:
            f.write(logs)
        
        return log_file
    except Exception as e:
        logger.error(f"Failed to save log to file: {str(e)}")
        raise

def upload_log_to_minio(log_file: str, namespace: str, pod_name: str) -> str:
    """å°†æ—¥å¿—æ–‡ä»¶ä¸Šä¼ åˆ° MinIO å¹¶è¿”å›ä¸‹è½½é“¾æ¥"""
    if not CONFIG.minio_enabled:
        logger.info("MinIO upload is disabled, skipping upload.")
        return None
    
    try:
        # åˆå§‹åŒ– MinIO å®¢æˆ·ç«¯
        minio_client = Minio(
            CONFIG.minio_endpoint,
            access_key=CONFIG.minio_access_key,
            secret_key=CONFIG.minio_secret_key,
            secure=CONFIG.minio_secure
        )
        
        # ç”Ÿæˆæ–‡ä»¶å¤¹å’Œæ–‡ä»¶å
        timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
        object_key = f"{CONFIG.cluster_name}/{namespace}/{pod_name}-{timestamp}.log"
        
        # æ£€æŸ¥æ¡¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
        if not minio_client.bucket_exists(CONFIG.minio_bucket):
            minio_client.make_bucket(CONFIG.minio_bucket)
        
        # ä¸Šä¼ æ–‡ä»¶
        minio_client.fput_object(CONFIG.minio_bucket, object_key, log_file)
        
        # ç”Ÿæˆä¸‹è½½é“¾æ¥
        if CONFIG.minio_proxy_endpoint:
            # ä½¿ç”¨ä»£ç†åçš„åœ°å€
            endpoint = CONFIG.minio_proxy_endpoint
        else:
            # ä½¿ç”¨åŸå§‹åœ°å€
            endpoint = CONFIG.minio_endpoint
        
        # æ ¹æ® minio_secure å†³å®šåè®®
        protocol = "https" if CONFIG.minio_secure else "http"
        return f"{protocol}://{endpoint}/{CONFIG.minio_bucket}/{object_key}"
    except S3Error as e:
        logger.error(f"Failed to upload log to MinIO: {str(e)}")
        raise
    except Exception as e:
        logger.error(f"Unexpected error during MinIO upload: {str(e)}")
        raise

def send_alert(data: dict):
    """å‘é€å‘Šè­¦"""
    try:
        # è®°å½•æ—¥å¿—
        logger.warning(
            f"Alerting for pod {data['metadata']['name']} "
            f"in {data['metadata']['namespace']}. "
            f"Restart count: {data['restart_info']['count']}"
        )
        # æ–°å¢å­—æ®µæå–
        pod = data['metadata']
        node_name = data.get('node_name', 'Unknown')
        restart_info = data['restart_info']
        reasons = restart_info.get('reasons', [])
        # æ ¼å¼åŒ–æ—¶é—´
        restart_time = restart_info.get('last_restart_time')
        
        send_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        # ä¿å­˜æ—¥å¿—ä¸ºæ–‡ä»¶
        log_file = save_log_to_file(data['logs'], data['metadata']['name'])
        
        # ä¸Šä¼ æ—¥å¿—æ–‡ä»¶åˆ° MinIO
        log_url = upload_log_to_minio(log_file, data['metadata']['namespace'], data['metadata']['name'])
        
        # æ„é€  Markdown
        md_content = f"""
### ğŸš¨ Podå¼‚å¸¸é‡å¯å‘Šè­¦ - {CONFIG.cluster_name}

####  åŸºæœ¬ä¿¡æ¯
> **é›†ç¾¤**: **{CONFIG.cluster_name}**
> **å‘½åç©ºé—´**: `{pod['namespace']}`
> **Podåç§°**: `{pod['name']}`
> **è¿è¡ŒèŠ‚ç‚¹**: {node_name}
> **é‡å¯æ—¶é—´**: `{restart_time}`
> **å‘é€æ—¶é—´**: {send_time}

#### å¼‚å¸¸çŠ¶æ€
> **é‡å¯æ¬¡æ•°**: `{data['restart_info']['count'] or 'Unknown'}`
> **æœ€åé€€å‡ºç **: {data['restart_info']['exit_code'] or 'Unknown'}
> **æœ€åå®¹å™¨**: {data['restart_info']['reasons'][0]['container'] or 'Unknown'}

#### ç»ˆæ­¢åŸå› 
> `{format_reasons(data['restart_info']['reasons'])}`

#### æœ€è¿‘äº‹ä»¶
> {format_events(data['events'])}

#### å…³é”®æ—¥å¿—
> {format_logs(data['logs'])[-CONFIG.log_lines*50:]} 

> **æ›´å¤šæ—¥å¿—è¯·ä¸‹è½½ï¼š[ç‚¹å‡»è¿™é‡Œ]({log_url})**
        """
        #logger.warning(md_content)
        # æ£€æŸ¥æ¶ˆæ¯é•¿åº¦æ˜¯å¦è¶…é™
        if len(md_content.encode('utf-8')) > 4096:
            logger.warning("Markdown content exceeds 4096 bytes, truncating logs...")
            # æˆªæ–­æ—¥å¿—å†…å®¹
            logs = format_logs(data['logs'])[-CONFIG.log_lines*50:]
            max_log_length = 4096 - len(md_content) + len(logs)
            truncated_logs = logs[:max_log_length] + "\n[Logs truncated due to length limit]"
            md_content = md_content.replace(logs, truncated_logs)
        # å‘é€ Webhook
        resp = requests.post(
            CONFIG.webhook_url,
            json={"msgtype": "markdown", "markdown": {"content": md_content}},
            timeout=10
        )
        resp.raise_for_status()
        logger.info("Alert sent successfully")
        
    except Exception as e:
        logger.error(f"Failed to send alert: {str(e)}")
        raise  # ç¡®ä¿å¼‚å¸¸ä¼ æ’­ï¼Œä¾¿äºè°ƒè¯•

def watch_namespace(namespace: str):
    """ç›‘æ§æŒ‡å®šå‘½åç©ºé—´æˆ–æ‰€æœ‰å‘½åç©ºé—´"""
    v1 = client.CoreV1Api()
    watcher = watch.Watch()
    while True:
        try:
            if namespace:  # å¦‚æœæŒ‡å®šäº†å…·ä½“çš„å‘½åç©ºé—´
                logger.info(f"Starting watch on namespace: {namespace}")
                stream = watcher.stream(
                    v1.list_namespaced_pod,
                    namespace=namespace,
                    timeout_seconds=CONFIG.watch_timeout
                )
            else:  # å¦‚æœ namespace ä¸ºç©ºï¼Œç›‘å¬æ‰€æœ‰å‘½åç©ºé—´
                logger.info("Starting watch on all namespaces")
                stream = watcher.stream(
                    v1.list_pod_for_all_namespaces,
                    timeout_seconds=CONFIG.watch_timeout
                )

            for event in stream:
                if event["type"] == "MODIFIED":
                    if data := prepare_alert_data(event["object"]):
                        send_alert(data)
        
        except Exception as e:
            logger.error(f"Watch error: {str(e)}")
            time.sleep(5)


if __name__ == "__main__":
    load_k8s_config()
    # å¯åŠ¨ç›‘æ§çº¿ç¨‹
    threads = []
    # å¦‚æœæ²¡æœ‰æŒ‡å®šå‘½åç©ºé—´ï¼Œåˆ™ç›‘å¬æ‰€æœ‰å‘½åç©ºé—´
    namespaces = CONFIG.watched_namespaces if CONFIG.watched_namespaces else [""]
    for ns in namespaces:
        thread = threading.Thread(
            target=watch_namespace,
            args=(ns,),
            daemon=True
        )
        thread.start()
        threads.append(thread)
        #logger.info(f"Started watcher for namespace: {ns or 'all'}")

    # ä¿æŒä¸»çº¿ç¨‹å­˜æ´»
    try:
        while True:
            time.sleep(60)
    except KeyboardInterrupt:
        logger.info("Shutting down monitor")
